{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1602bdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wikipedia in c:\\data science\\gen ai\\langchain\\langchain models\\venv\\lib\\site-packages (1.4.0)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\data science\\gen ai\\langchain\\langchain models\\venv\\lib\\site-packages (from wikipedia) (4.13.5)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.0.0 in c:\\data science\\gen ai\\langchain\\langchain models\\venv\\lib\\site-packages (from wikipedia) (2.32.4)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\data science\\gen ai\\langchain\\langchain models\\venv\\lib\\site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\data science\\gen ai\\langchain\\langchain models\\venv\\lib\\site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\data science\\gen ai\\langchain\\langchain models\\venv\\lib\\site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\data science\\gen ai\\langchain\\langchain models\\venv\\lib\\site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2025.8.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\data science\\gen ai\\langchain\\langchain models\\venv\\lib\\site-packages (from beautifulsoup4->wikipedia) (2.7)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in c:\\data science\\gen ai\\langchain\\langchain models\\venv\\lib\\site-packages (from beautifulsoup4->wikipedia) (4.14.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "! \"c:\\DATA SCIENCE\\GEN AI\\LangChain\\Langchain Models\\venv\\Scripts\\python.exe\" -m pip install sentence-transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "afd23960",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from dotenv import load_dotenv\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "a35b1bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "# Initialize LLM\n",
    "model = ChatGroq(\n",
    "    model=\"meta-llama/llama-4-scout-17b-16e-instruct\",\n",
    "    temperature=0.5,\n",
    "    max_tokens=512,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "ccddda67",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = f\"https://python.langchain.com/docs/integrations/tools/\"\n",
    "\n",
    "loader = WebBaseLoader(url)\n",
    "\n",
    "document = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "ba2dea2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 300,\n",
    "    chunk_overlap = 50,\n",
    ")\n",
    "\n",
    "docs = splitter.split_documents(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "736e6635",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize embeddings\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"BAAI/bge-small-en-v1.5\")\n",
    "\n",
    "# Create FAISS vector store from documents\n",
    "vector_store = FAISS.from_documents(docs, embedding=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "fe7598a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vector_store.as_retriever(\n",
    "    search_type=\"mmr\",                   # <-- This enables MMR\n",
    "    search_kwargs={\"k\": 3, \"lambda_mult\": 0.5}  # k = top results, lambda_mult = relevance-diversity balance\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "3d9ce9a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You: \n",
      "AI: I'm ready to help. What is your question? \n",
      "\n",
      "(Please go ahead and ask your question, and I'll answer based on the webpage text provided)\n",
      "Thank you, see you soon ðŸ‘‹\n"
     ]
    }
   ],
   "source": [
    "qa_prompt = PromptTemplate(\n",
    "    template=\"\"\"\n",
    "You are a helpful assistant that answers questions based on a webpage.\n",
    "\n",
    "Conversation so far:\n",
    "{history}\n",
    "\n",
    "Here is the webpage text (retrieved context):\n",
    "----------------\n",
    "{context}\n",
    "----------------\n",
    "\n",
    "Now, using ONLY the webpage text above:\n",
    "Q: {question}\n",
    "A:\"\"\",\n",
    "    input_variables=[\"history\", \"context\", \"question\"]\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Maintain chat history\n",
    "messages = []\n",
    "\n",
    "# Chat loop\n",
    "while True:\n",
    "    query = input(\"You: \")\n",
    "    if query.lower() == \"exit\":\n",
    "        print(\"Thank you, see you soon ðŸ‘‹\")\n",
    "        break\n",
    "    print(f\"You: {query}\")\n",
    "    # Append user message\n",
    "    messages.append({\"role\": \"user\", \"content\": query})\n",
    "\n",
    "    # Build history context\n",
    "    history_context = \"\\n\".join(\n",
    "        [f\"{m['role'].capitalize()}: {m['content']}\" for m in messages]\n",
    "    )\n",
    "\n",
    "    # Retrieve relevant docs from vector DB (wiki/web)\n",
    "    results = retriever.invoke(query)\n",
    "    docs_context = \"\\n\".join([doc.page_content for doc in results])\n",
    "\n",
    "    # Final context (history + docs)\n",
    "    formatted_prompt = qa_prompt.format(\n",
    "    history=history_context,\n",
    "    context=docs_context if docs_context.strip() else \"No relevant text found.\",\n",
    "    question=query\n",
    "    )\n",
    "\n",
    "    # Call model directly (LLM)\n",
    "    ans = model.invoke(formatted_prompt)\n",
    "\n",
    "    # âœ… Fix: extract content properly\n",
    "    response = ans.content\n",
    "\n",
    "    # Append AI message\n",
    "    messages.append({\"role\": \"assistant\", \"content\": response})\n",
    "\n",
    "    print(f\"AI: {response}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a14ce1a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
